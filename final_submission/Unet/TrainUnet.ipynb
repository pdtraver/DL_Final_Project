{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed6fadb-94ac-4ecf-a844-79428a3a76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4279a5fd-e1f6-4f0b-aec2-f2d6d2ecaa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA for tensor operations\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Set the default device to CUDA\n",
    "    device = torch.device('cuda')\n",
    "    torch.set_default_device(device)\n",
    "    print('Using CUDA for tensor operations')\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print('CUDA is not available. Using CPU for tensor operations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "341c9988-dfc1-406c-8541-1066f20e39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLEVRDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.video_paths = [os.path.join(path, dir_path) for dir_path in os.listdir(path) if dir_path.startswith('video')]\n",
    "        self.transform = transform\n",
    "        self._get_num_samples()\n",
    "        \n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),  # Converts PIL Image to Tensor.\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Standard normalization for pre-trained models.\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.ToTensor()  # Only convert masks to tensor without normalization.\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def _get_num_samples(self):\n",
    "        mask_sample_path = os.path.join(self.video_paths[0], 'mask.npy')\n",
    "        mask_shapes = np.load(mask_sample_path).shape\n",
    "        self.folder_num_samples = mask_shapes[0]\n",
    "        self.num_samples = self.folder_num_samples * len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_index = idx % self.folder_num_samples\n",
    "        folder_index = int(idx/self.folder_num_samples)\n",
    "        \n",
    "        img_name = os.path.join(self.video_paths[folder_index], f'image_{image_index}.png')\n",
    "        mask_name = os.path.join(self.video_paths[folder_index], 'mask.npy')\n",
    "        \n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        mask = np.load(mask_name)[image_index]\n",
    "        \n",
    "        # Create a one-hot encoded tensor\n",
    "        num_objects = 49\n",
    "        mask_tensor = torch.from_numpy(mask)\n",
    "        height, width = mask_tensor.shape\n",
    "        one_hot_mask = torch.zeros((num_objects, height, width), dtype=torch.float, device=device)\n",
    "        one_hot_mask.scatter_(0, mask_tensor.unsqueeze(0).to(torch.int64).to(device), 1)\n",
    "\n",
    "        image = self.transform(image).to(device)\n",
    "\n",
    "        return image, one_hot_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be95983-b36f-4725-ae4f-acb1378564aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        # Encoder\n",
    "        self.down1 = self.contract_block(3, 64, 7, 3)\n",
    "        self.down2 = self.contract_block(64, 128, 3, 1)\n",
    "        self.down3 = self.contract_block(128, 256, 3, 1)\n",
    "        self.down4 = self.contract_block(256, 512, 3, 1)\n",
    "\n",
    "        # Decoder\n",
    "        self.up3 = self.expand_block(512, 256, 3, 1)\n",
    "        self.up2 = self.expand_block(256, 128, 3, 1)\n",
    "        self.up1 = self.expand_block(128, 64, 3, 1)\n",
    "        self.final_up = nn.ConvTranspose2d(64, 49, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.final = nn.Conv2d(49, 49, kernel_size=1)  # Change from 1 to 48\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.up3(x4)\n",
    "        x = self.up2(x + x3)\n",
    "        x = self.up1(x + x2)\n",
    "        x = self.final_up(x + x1)\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "        contract = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        return contract\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "        expand = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        return expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19e4f08-cbb5-49db-b01b-d6198440d38b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] Batch Loss: 0.14566248\n",
      "[1,   200] Batch Loss: 0.01025905\n",
      "[1,   300] Batch Loss: 0.00758162\n",
      "Epoch 1, Total Loss: 16.68228873424232\n",
      "[2,   100] Batch Loss: 0.00599147\n",
      "[2,   200] Batch Loss: 0.00558053\n",
      "[2,   300] Batch Loss: 0.00458001\n",
      "Epoch 2, Total Loss: 1.8149344213306904\n",
      "[3,   100] Batch Loss: 0.00414734\n",
      "[3,   200] Batch Loss: 0.00370698\n",
      "[3,   300] Batch Loss: 0.00368675\n",
      "Epoch 3, Total Loss: 1.3038259758614004\n",
      "[4,   100] Batch Loss: 0.00293263\n",
      "[4,   200] Batch Loss: 0.00279254\n",
      "[4,   300] Batch Loss: 0.00258937\n",
      "Epoch 4, Total Loss: 0.9451475997921079\n",
      "[5,   100] Batch Loss: 0.00229698\n",
      "[5,   200] Batch Loss: 0.00212843\n",
      "[5,   300] Batch Loss: 0.00194772\n",
      "Epoch 5, Total Loss: 0.7162891470361501\n",
      "[6,   100] Batch Loss: 0.00150876\n",
      "[6,   200] Batch Loss: 0.00126956\n",
      "[6,   300] Batch Loss: 0.00104856\n",
      "Epoch 6, Total Loss: 0.42119537241524085\n",
      "[7,   100] Batch Loss: 0.00080216\n",
      "[7,   200] Batch Loss: 0.00070042\n",
      "[7,   300] Batch Loss: 0.00063050\n",
      "Epoch 7, Total Loss: 0.24150251364335418\n",
      "[8,   100] Batch Loss: 0.00054114\n",
      "[8,   200] Batch Loss: 0.00055128\n",
      "[8,   300] Batch Loss: 0.00048609\n",
      "Epoch 8, Total Loss: 0.17883000479196198\n",
      "[9,   100] Batch Loss: 0.00042892\n",
      "[9,   200] Batch Loss: 0.00042032\n",
      "[9,   300] Batch Loss: 0.00040058\n",
      "Epoch 9, Total Loss: 0.1419679934042506\n",
      "[10,   100] Batch Loss: 0.00037815\n",
      "[10,   200] Batch Loss: 0.00037139\n",
      "[10,   300] Batch Loss: 0.00035608\n",
      "Epoch 10, Total Loss: 0.1257935372123029\n",
      "[11,   100] Batch Loss: 0.00033171\n",
      "[11,   200] Batch Loss: 0.00033200\n",
      "[11,   300] Batch Loss: 0.00031518\n",
      "Epoch 11, Total Loss: 0.11225087926140986\n",
      "[12,   100] Batch Loss: 0.00033285\n",
      "[12,   200] Batch Loss: 0.00037383\n",
      "[12,   300] Batch Loss: 0.00031066\n",
      "Epoch 12, Total Loss: 0.11513509099313524\n",
      "[13,   100] Batch Loss: 0.00028196\n",
      "[13,   200] Batch Loss: 0.00027089\n",
      "[13,   300] Batch Loss: 0.00027970\n",
      "Epoch 13, Total Loss: 0.09704550069000106\n",
      "[14,   100] Batch Loss: 0.00028147\n",
      "[14,   200] Batch Loss: 0.00025430\n",
      "[14,   300] Batch Loss: 0.00029091\n",
      "Epoch 14, Total Loss: 0.09417201935139019\n",
      "[15,   100] Batch Loss: 0.00023404\n",
      "[15,   200] Batch Loss: 0.00023320\n",
      "[15,   300] Batch Loss: 0.00023662\n",
      "Epoch 15, Total Loss: 0.08093708858359605\n",
      "[16,   100] Batch Loss: 0.00022141\n",
      "[16,   200] Batch Loss: 0.00022561\n",
      "[16,   300] Batch Loss: 0.00025996\n",
      "Epoch 16, Total Loss: 0.08217946020886302\n",
      "[17,   100] Batch Loss: 0.00028547\n",
      "[17,   200] Batch Loss: 0.00026790\n",
      "[17,   300] Batch Loss: 0.00022759\n",
      "Epoch 17, Total Loss: 0.08827757614199072\n",
      "[18,   100] Batch Loss: 0.00020989\n",
      "[18,   200] Batch Loss: 0.00019925\n",
      "[18,   300] Batch Loss: 0.00021091\n",
      "Epoch 18, Total Loss: 0.07117576083692256\n",
      "[19,   100] Batch Loss: 0.00022257\n",
      "[19,   200] Batch Loss: 0.00021991\n",
      "[19,   300] Batch Loss: 0.00020369\n",
      "Epoch 19, Total Loss: 0.07507135618652683\n",
      "[20,   100] Batch Loss: 0.00020927\n",
      "[20,   200] Batch Loss: 0.00019143\n",
      "[20,   300] Batch Loss: 0.00019184\n",
      "Epoch 20, Total Loss: 0.06773814772896003\n",
      "[21,   100] Batch Loss: 0.00017614\n",
      "[21,   200] Batch Loss: 0.00018726\n",
      "[21,   300] Batch Loss: 0.00017799\n",
      "Epoch 21, Total Loss: 0.06195087142987177\n",
      "[22,   100] Batch Loss: 0.00018282\n",
      "[22,   200] Batch Loss: 0.00025313\n",
      "[22,   300] Batch Loss: 0.00026799\n",
      "Epoch 22, Total Loss: 0.08065588895988185\n",
      "[23,   100] Batch Loss: 0.00019422\n",
      "[23,   200] Batch Loss: 0.00017977\n",
      "[23,   300] Batch Loss: 0.00018058\n",
      "Epoch 23, Total Loss: 0.06332218984607607\n",
      "[24,   100] Batch Loss: 0.00015753\n",
      "[24,   200] Batch Loss: 0.00015614\n",
      "[24,   300] Batch Loss: 0.00015779\n",
      "Epoch 24, Total Loss: 0.0542400547710713\n",
      "[25,   100] Batch Loss: 0.00014631\n",
      "[25,   200] Batch Loss: 0.00014795\n",
      "[25,   300] Batch Loss: 0.00014879\n",
      "Epoch 25, Total Loss: 0.05111086252145469\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 25\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CLEVRDataset(path='dataset/train/')\n",
    "\n",
    "# Assuming 'dataset' is already defined\n",
    "generator = torch.Generator(device='cuda')\n",
    "sampler = RandomSampler(dataset, generator=generator)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler, shuffle=(sampler is None))\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "model = UNet().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    batch = 0\n",
    "    total_loss = 0\n",
    "    batch_loss = 0\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "        batch += 1\n",
    "        \n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f'[{epoch + 1}, {batch + 1:5d}] Batch Loss: {batch_loss / 100:.8f}')\n",
    "            batch_loss = 0.0\n",
    "            \n",
    "    print(f'Epoch {epoch+1}, Total Loss: {total_loss}')\n",
    "    if (epoch+1)%5 == 0:\n",
    "        checkpoint_path = f\"Unet_checkpoint_epoch_{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b019402b-3f32-4878-ba49-2553813557f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26,   100] Batch Loss: 0.00013680\n",
      "[26,   200] Batch Loss: 0.00014168\n",
      "[26,   300] Batch Loss: 0.00014591\n",
      "Epoch 26, Total Loss: 0.0510188798289164\n",
      "[27,   100] Batch Loss: 0.00036189\n",
      "[27,   200] Batch Loss: 0.00023557\n",
      "[27,   300] Batch Loss: 0.00018590\n",
      "Epoch 27, Total Loss: 0.08608283301873598\n",
      "[28,   100] Batch Loss: 0.00015653\n",
      "[28,   200] Batch Loss: 0.00015898\n",
      "[28,   300] Batch Loss: 0.00015448\n",
      "Epoch 28, Total Loss: 0.053673918460845016\n",
      "[29,   100] Batch Loss: 0.00014426\n",
      "[29,   200] Batch Loss: 0.00014535\n",
      "[29,   300] Batch Loss: 0.00013715\n",
      "Epoch 29, Total Loss: 0.04875242297566729\n",
      "[30,   100] Batch Loss: 0.00012515\n",
      "[30,   200] Batch Loss: 0.00012648\n",
      "[30,   300] Batch Loss: 0.00013156\n",
      "Epoch 30, Total Loss: 0.04430344058346236\n",
      "[31,   100] Batch Loss: 0.00011842\n",
      "[31,   200] Batch Loss: 0.00012007\n",
      "[31,   300] Batch Loss: 0.00012235\n",
      "Epoch 31, Total Loss: 0.04170447447540937\n",
      "[32,   100] Batch Loss: 0.00011385\n",
      "[32,   200] Batch Loss: 0.00012223\n",
      "[32,   300] Batch Loss: 0.00012201\n",
      "Epoch 32, Total Loss: 0.04141569454804994\n",
      "[33,   100] Batch Loss: 0.00011717\n",
      "[33,   200] Batch Loss: 0.00025021\n",
      "[33,   300] Batch Loss: 0.00024427\n",
      "Epoch 33, Total Loss: 0.06921050739038037\n",
      "[34,   100] Batch Loss: 0.00014285\n",
      "[34,   200] Batch Loss: 0.00014211\n",
      "[34,   300] Batch Loss: 0.00013178\n",
      "Epoch 34, Total Loss: 0.05075562700221781\n",
      "[35,   100] Batch Loss: 0.00014514\n",
      "[35,   200] Batch Loss: 0.00012817\n",
      "[35,   300] Batch Loss: 0.00012420\n",
      "Epoch 35, Total Loss: 0.04516943735507084\n",
      "[36,   100] Batch Loss: 0.00011291\n",
      "[36,   200] Batch Loss: 0.00011473\n",
      "[36,   300] Batch Loss: 0.00011167\n",
      "Epoch 36, Total Loss: 0.03904253015934955\n",
      "[37,   100] Batch Loss: 0.00010266\n",
      "[37,   200] Batch Loss: 0.00010503\n",
      "[37,   300] Batch Loss: 0.00010611\n",
      "Epoch 37, Total Loss: 0.036088060987822246\n",
      "[38,   100] Batch Loss: 0.00009535\n",
      "[38,   200] Batch Loss: 0.00010000\n",
      "[38,   300] Batch Loss: 0.00010142\n",
      "Epoch 38, Total Loss: 0.034373586961010005\n",
      "[39,   100] Batch Loss: 0.00009463\n",
      "[39,   200] Batch Loss: 0.00009708\n",
      "[39,   300] Batch Loss: 0.00012343\n",
      "Epoch 39, Total Loss: 0.03863088104117196\n",
      "[40,   100] Batch Loss: 0.00019533\n",
      "[40,   200] Batch Loss: 0.00020007\n",
      "[40,   300] Batch Loss: 0.00016657\n",
      "Epoch 40, Total Loss: 0.06221609225758584\n",
      "[41,   100] Batch Loss: 0.00011108\n",
      "[41,   200] Batch Loss: 0.00011053\n",
      "[41,   300] Batch Loss: 0.00010793\n",
      "Epoch 41, Total Loss: 0.037678944412618876\n",
      "[42,   100] Batch Loss: 0.00009305\n",
      "[42,   200] Batch Loss: 0.00009651\n",
      "[42,   300] Batch Loss: 0.00009639\n",
      "Epoch 42, Total Loss: 0.0329666998659377\n",
      "[43,   100] Batch Loss: 0.00009278\n",
      "[43,   200] Batch Loss: 0.00009212\n",
      "[43,   300] Batch Loss: 0.00009136\n",
      "Epoch 43, Total Loss: 0.03183655586326495\n",
      "[44,   100] Batch Loss: 0.00008336\n",
      "[44,   200] Batch Loss: 0.00008645\n",
      "[44,   300] Batch Loss: 0.00008762\n",
      "Epoch 44, Total Loss: 0.02976795055292314\n",
      "[45,   100] Batch Loss: 0.00008141\n",
      "[45,   200] Batch Loss: 0.00008388\n",
      "[45,   300] Batch Loss: 0.00008566\n",
      "Epoch 45, Total Loss: 0.029051946410618257\n",
      "[46,   100] Batch Loss: 0.00007858\n",
      "[46,   200] Batch Loss: 0.00008371\n",
      "[46,   300] Batch Loss: 0.00008588\n",
      "Epoch 46, Total Loss: 0.02875849194242619\n",
      "[47,   100] Batch Loss: 0.00007886\n",
      "[47,   200] Batch Loss: 0.00008365\n",
      "[47,   300] Batch Loss: 0.00008454\n",
      "Epoch 47, Total Loss: 0.028472631849581376\n",
      "[48,   100] Batch Loss: 0.00007759\n",
      "[48,   200] Batch Loss: 0.00007945\n",
      "[48,   300] Batch Loss: 0.00008275\n",
      "Epoch 48, Total Loss: 0.027813296743261162\n",
      "[49,   100] Batch Loss: 0.00007503\n",
      "[49,   200] Batch Loss: 0.00007630\n",
      "[49,   300] Batch Loss: 0.00007839\n",
      "Epoch 49, Total Loss: 0.026564364656223916\n",
      "[50,   100] Batch Loss: 0.00007919\n",
      "[50,   200] Batch Loss: 0.00009918\n",
      "[50,   300] Batch Loss: 0.00009795\n",
      "Epoch 50, Total Loss: 0.0323235134492279\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(num_epochs, 2*num_epochs):\n",
    "    batch = 0\n",
    "    total_loss = 0\n",
    "    batch_loss = 0\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "        batch += 1\n",
    "        \n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f'[{epoch + 1}, {batch + 1:5d}] Batch Loss: {batch_loss / 100:.8f}')\n",
    "            batch_loss = 0.0\n",
    "            \n",
    "    print(f'Epoch {epoch+1}, Total Loss: {total_loss}')\n",
    "    if (epoch+1)%5 == 0:\n",
    "        checkpoint_path = f\"Unet_checkpoint_epoch_{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fee5922-0399-4877-bf0d-8d4c111ecc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51,   100] Batch Loss: 0.00008873\n",
      "[51,   200] Batch Loss: 0.00023209\n",
      "[51,   300] Batch Loss: 0.00020863\n",
      "Epoch 51, Total Loss: 0.059385071726865135\n",
      "[52,   100] Batch Loss: 0.00010968\n",
      "[52,   200] Batch Loss: 0.00010077\n",
      "[52,   300] Batch Loss: 0.00009586\n",
      "Epoch 52, Total Loss: 0.034787286844220944\n",
      "[53,   100] Batch Loss: 0.00007949\n",
      "[53,   200] Batch Loss: 0.00007960\n",
      "Epoch 53, Total Loss: 0.027653157674649265\n",
      "[54,   100] Batch Loss: 0.00007219\n",
      "[54,   200] Batch Loss: 0.00007236\n",
      "[54,   300] Batch Loss: 0.00007830\n",
      "Epoch 54, Total Loss: 0.025752232009836007\n",
      "[55,   100] Batch Loss: 0.00006736\n",
      "[55,   200] Batch Loss: 0.00006957\n",
      "[55,   300] Batch Loss: 0.00007130\n",
      "Epoch 55, Total Loss: 0.024036996401264332\n",
      "[56,   100] Batch Loss: 0.00006458\n",
      "[56,   200] Batch Loss: 0.00006646\n",
      "[56,   300] Batch Loss: 0.00006795\n",
      "Epoch 56, Total Loss: 0.022954800544539467\n",
      "[57,   100] Batch Loss: 0.00006182\n",
      "[57,   200] Batch Loss: 0.00006508\n",
      "[57,   300] Batch Loss: 0.00006667\n",
      "Epoch 57, Total Loss: 0.022395457213860936\n",
      "[58,   100] Batch Loss: 0.00006078\n",
      "[58,   200] Batch Loss: 0.00006393\n",
      "[58,   300] Batch Loss: 0.00006589\n",
      "Epoch 58, Total Loss: 0.02207295410335064\n",
      "[59,   100] Batch Loss: 0.00006059\n",
      "[59,   200] Batch Loss: 0.00006277\n",
      "[59,   300] Batch Loss: 0.00006495\n",
      "Epoch 59, Total Loss: 0.021841326175490394\n",
      "[60,   100] Batch Loss: 0.00008313\n",
      "[60,   200] Batch Loss: 0.00007365\n",
      "[60,   300] Batch Loss: 0.00006656\n",
      "Epoch 60, Total Loss: 0.025382152904057875\n",
      "[61,   100] Batch Loss: 0.00006564\n",
      "[61,   200] Batch Loss: 0.00007988\n",
      "[61,   300] Batch Loss: 0.00006847\n",
      "Epoch 61, Total Loss: 0.024407436038018204\n",
      "[62,   100] Batch Loss: 0.00005866\n",
      "[62,   200] Batch Loss: 0.00006053\n",
      "[62,   300] Batch Loss: 0.00006259\n",
      "Epoch 62, Total Loss: 0.0210071421606699\n",
      "[63,   100] Batch Loss: 0.00005546\n",
      "[63,   200] Batch Loss: 0.00005761\n",
      "[63,   300] Batch Loss: 0.00006067\n",
      "Epoch 63, Total Loss: 0.020176654787064763\n",
      "[64,   100] Batch Loss: 0.00005566\n",
      "[64,   200] Batch Loss: 0.00005676\n",
      "[64,   300] Batch Loss: 0.00005852\n",
      "Epoch 64, Total Loss: 0.019927133915189188\n",
      "[65,   100] Batch Loss: 0.00005531\n",
      "[65,   200] Batch Loss: 0.00005589\n",
      "[65,   300] Batch Loss: 0.00005736\n",
      "Epoch 65, Total Loss: 0.019513705432473216\n",
      "[66,   100] Batch Loss: 0.00005304\n",
      "[66,   200] Batch Loss: 0.00005583\n",
      "[66,   300] Batch Loss: 0.00005694\n",
      "Epoch 66, Total Loss: 0.01920805495683453\n",
      "[67,   100] Batch Loss: 0.00005220\n",
      "[67,   200] Batch Loss: 0.00005396\n",
      "[67,   300] Batch Loss: 0.00005610\n",
      "Epoch 67, Total Loss: 0.01883300274494104\n",
      "[68,   100] Batch Loss: 0.00005148\n",
      "[68,   200] Batch Loss: 0.00005382\n",
      "[68,   300] Batch Loss: 0.00005455\n",
      "Epoch 68, Total Loss: 0.018462825948518002\n",
      "[69,   100] Batch Loss: 0.00005146\n",
      "[69,   200] Batch Loss: 0.00005153\n",
      "[69,   300] Batch Loss: 0.00005407\n",
      "Epoch 69, Total Loss: 0.018270836135343416\n",
      "[70,   100] Batch Loss: 0.00005062\n",
      "[70,   200] Batch Loss: 0.00005109\n",
      "[70,   300] Batch Loss: 0.00005462\n",
      "Epoch 70, Total Loss: 0.019991797911643516\n",
      "[71,   100] Batch Loss: 0.00009873\n",
      "[71,   200] Batch Loss: 0.00022418\n",
      "[71,   300] Batch Loss: 0.00017625\n",
      "Epoch 71, Total Loss: 0.05561987443797989\n",
      "[72,   100] Batch Loss: 0.00009708\n",
      "[72,   200] Batch Loss: 0.00008909\n",
      "[72,   300] Batch Loss: 0.00007800\n",
      "Epoch 72, Total Loss: 0.029788346793793608\n",
      "[73,   100] Batch Loss: 0.00006232\n",
      "[73,   200] Batch Loss: 0.00006304\n",
      "[73,   300] Batch Loss: 0.00006175\n",
      "Epoch 73, Total Loss: 0.02146154860747629\n",
      "[74,   100] Batch Loss: 0.00005158\n",
      "[74,   200] Batch Loss: 0.00005332\n",
      "[74,   300] Batch Loss: 0.00005489\n",
      "Epoch 74, Total Loss: 0.018459329770848854\n",
      "[75,   100] Batch Loss: 0.00004852\n",
      "[75,   200] Batch Loss: 0.00004929\n",
      "[75,   300] Batch Loss: 0.00005123\n",
      "Epoch 75, Total Loss: 0.017200963626237353\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(2*num_epochs, 3*num_epochs):\n",
    "    batch = 0\n",
    "    total_loss = 0\n",
    "    batch_loss = 0\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "        batch += 1\n",
    "        \n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f'[{epoch + 1}, {batch + 1:5d}] Batch Loss: {batch_loss / 100:.8f}')\n",
    "            batch_loss = 0.0\n",
    "            \n",
    "    print(f'Epoch {epoch+1}, Total Loss: {total_loss}')\n",
    "    if (epoch+1)%5 == 0:\n",
    "        checkpoint_path = f\"Unet_checkpoint_epoch_{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9415ae91-c731-47fd-8f54-a5a71236594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76,   100] Batch Loss: 0.00004523\n",
      "[76,   200] Batch Loss: 0.00004824\n",
      "[76,   300] Batch Loss: 0.00004951\n",
      "Epoch 76, Total Loss: 0.01655479281907901\n",
      "[77,   100] Batch Loss: 0.00004516\n",
      "[77,   200] Batch Loss: 0.00004728\n",
      "[77,   300] Batch Loss: 0.00004798\n",
      "Epoch 77, Total Loss: 0.016305480243318016\n",
      "[78,   100] Batch Loss: 0.00004425\n",
      "[78,   200] Batch Loss: 0.00004574\n",
      "[78,   300] Batch Loss: 0.00004803\n",
      "Epoch 78, Total Loss: 0.015993970846466254\n",
      "[79,   100] Batch Loss: 0.00004355\n",
      "[79,   200] Batch Loss: 0.00004525\n",
      "[79,   300] Batch Loss: 0.00004676\n",
      "Epoch 79, Total Loss: 0.015698602594056865\n",
      "[80,   100] Batch Loss: 0.00004308\n",
      "[80,   200] Batch Loss: 0.00004531\n",
      "[80,   300] Batch Loss: 0.00004671\n",
      "Epoch 80, Total Loss: 0.01559824840296642\n",
      "[81,   100] Batch Loss: 0.00004339\n",
      "[81,   200] Batch Loss: 0.00004550\n",
      "[81,   300] Batch Loss: 0.00004633\n",
      "Epoch 81, Total Loss: 0.015680738448281772\n",
      "[82,   100] Batch Loss: 0.00004224\n",
      "[82,   200] Batch Loss: 0.00004408\n",
      "[82,   300] Batch Loss: 0.00004559\n",
      "Epoch 82, Total Loss: 0.015231873985612765\n",
      "[83,   100] Batch Loss: 0.00004191\n",
      "[83,   200] Batch Loss: 0.00004270\n",
      "[83,   300] Batch Loss: 0.00004593\n",
      "Epoch 83, Total Loss: 0.015103468562301714\n",
      "[84,   100] Batch Loss: 0.00004061\n",
      "[84,   200] Batch Loss: 0.00004346\n",
      "[84,   300] Batch Loss: 0.00004457\n",
      "Epoch 84, Total Loss: 0.014954951704567065\n",
      "[85,   100] Batch Loss: 0.00004079\n",
      "[85,   200] Batch Loss: 0.00004307\n",
      "[85,   300] Batch Loss: 0.00004401\n",
      "Epoch 85, Total Loss: 0.014767275424674153\n",
      "[86,   100] Batch Loss: 0.00003972\n",
      "[86,   200] Batch Loss: 0.00004178\n",
      "[86,   300] Batch Loss: 0.00004325\n",
      "Epoch 86, Total Loss: 0.014459550842730096\n",
      "[87,   100] Batch Loss: 0.00004023\n",
      "[87,   200] Batch Loss: 0.00004087\n",
      "[87,   300] Batch Loss: 0.00004297\n",
      "Epoch 87, Total Loss: 0.01436129147259635\n",
      "[88,   100] Batch Loss: 0.00003971\n",
      "[88,   200] Batch Loss: 0.00004089\n",
      "[88,   300] Batch Loss: 0.00004266\n",
      "Epoch 88, Total Loss: 0.014264387351431651\n",
      "[89,   100] Batch Loss: 0.00003895\n",
      "[89,   200] Batch Loss: 0.00004081\n",
      "[89,   300] Batch Loss: 0.00004173\n",
      "Epoch 89, Total Loss: 0.014137476679024985\n",
      "[90,   100] Batch Loss: 0.00003907\n",
      "[90,   200] Batch Loss: 0.00003999\n",
      "[90,   300] Batch Loss: 0.00004067\n",
      "Epoch 90, Total Loss: 0.013795745206152787\n",
      "[91,   100] Batch Loss: 0.00003729\n",
      "[91,   200] Batch Loss: 0.00003880\n",
      "[91,   300] Batch Loss: 0.00004056\n",
      "Epoch 91, Total Loss: 0.013548336672101868\n",
      "[92,   100] Batch Loss: 0.00003677\n",
      "[92,   200] Batch Loss: 0.00003968\n",
      "[92,   300] Batch Loss: 0.00004023\n",
      "Epoch 92, Total Loss: 0.013512783665646566\n",
      "[93,   100] Batch Loss: 0.00003740\n",
      "[93,   200] Batch Loss: 0.00003951\n",
      "[93,   300] Batch Loss: 0.00004108\n",
      "Epoch 93, Total Loss: 0.014401299005839974\n",
      "[94,   100] Batch Loss: 0.00006663\n",
      "[94,   200] Batch Loss: 0.00010926\n",
      "[94,   300] Batch Loss: 0.00006870\n",
      "Epoch 94, Total Loss: 0.0270269694228773\n",
      "[95,   100] Batch Loss: 0.00004717\n",
      "[95,   200] Batch Loss: 0.00004630\n",
      "[95,   300] Batch Loss: 0.00004443\n",
      "Epoch 95, Total Loss: 0.01572579585990752\n",
      "[96,   100] Batch Loss: 0.00003640\n",
      "[96,   200] Batch Loss: 0.00003703\n",
      "[96,   300] Batch Loss: 0.00003731\n",
      "Epoch 96, Total Loss: 0.012801145800040103\n",
      "[97,   100] Batch Loss: 0.00003361\n",
      "[97,   200] Batch Loss: 0.00003497\n",
      "[97,   300] Batch Loss: 0.00003577\n",
      "Epoch 97, Total Loss: 0.01204805356246652\n",
      "[98,   100] Batch Loss: 0.00003257\n",
      "[98,   200] Batch Loss: 0.00003416\n",
      "[98,   300] Batch Loss: 0.00003557\n",
      "Epoch 98, Total Loss: 0.011816874395663035\n",
      "[99,   100] Batch Loss: 0.00003234\n",
      "[99,   200] Batch Loss: 0.00003529\n",
      "[99,   300] Batch Loss: 0.00003571\n",
      "Epoch 99, Total Loss: 0.012073306976162712\n",
      "[100,   100] Batch Loss: 0.00003658\n",
      "[100,   200] Batch Loss: 0.00004358\n",
      "[100,   300] Batch Loss: 0.00016042\n",
      "Epoch 100, Total Loss: 0.02864493360357301\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(3*num_epochs, 4*num_epochs):\n",
    "    batch = 0\n",
    "    total_loss = 0\n",
    "    batch_loss = 0\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        batch_loss += loss.item()\n",
    "        \n",
    "        batch += 1\n",
    "        \n",
    "        if (batch + 1) % 100 == 0:\n",
    "            print(f'[{epoch + 1}, {batch + 1:5d}] Batch Loss: {batch_loss / 100:.8f}')\n",
    "            batch_loss = 0.0\n",
    "            \n",
    "    print(f'Epoch {epoch+1}, Total Loss: {total_loss}')\n",
    "    if (epoch+1)%5 == 0:\n",
    "        checkpoint_path = f\"Unet_checkpoint_epoch_{epoch}.pt\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
