{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27f82f5e-82dc-49a7-805a-023e57e5c65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe52dfe-e4e6-4c1b-9ab9-57ed8c6dba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelDataset(tf.data.Dataset):\n",
    "    def __init__(self, path, batch_size):\n",
    "        self.video_paths = [os.path.join(path, dir_path) for dir_path in os.listdir(path) if dir_path.startswith('video')]\n",
    "        self.num_samples = 0\n",
    "\n",
    "        # Load the mask shape from the first video path\n",
    "        mask_sample_path = os.path.join(self.video_paths[0], 'mask.npy')\n",
    "        mask_shapes = np.load(mask_sample_path).shape\n",
    "\n",
    "        # Calculate the total number of samples\n",
    "        self.num_samples = mask_shapes[0] * len(self.video_paths)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _inputs(self):\n",
    "        return []\n",
    "\n",
    "    def element_spec(self):\n",
    "        image_shape = tf.TensorShape([None, None, 3])\n",
    "        mask_shape = tf.TensorShape([None])\n",
    "        return (image_shape, mask_shape)\n",
    "\n",
    "    def _generator(self):\n",
    "        images_batch = []\n",
    "        masks_batch = []\n",
    "        for path in self.video_paths:\n",
    "            mask_path = os.path.join(path, 'mask.npy')\n",
    "            masks = np.load(mask_path)\n",
    "\n",
    "            for i in range(masks.shape[0]): \n",
    "                image_path = os.path.join(path, f'image_{i}.png')\n",
    "                image_data = tf.io.read_file(image_path)\n",
    "\n",
    "                image = tf.image.decode_png(image_data, channels=3)\n",
    "                #normalize\n",
    "                image = tf.cast(image, tf.float32)\n",
    "                image = ((image / 255.0) - 0.5) * 2.0  # Rescale to [-1, 1].\n",
    "                image = tf.image.resize(\n",
    "                    image, (160,240), method=tf.image.ResizeMethod.BILINEAR)\n",
    "                image = tf.clip_by_value(image, -1., 1.)\n",
    "                \n",
    "                mask = tf.convert_to_tensor(masks[i])\n",
    "\n",
    "                images_batch.append(image)\n",
    "                masks_batch.append(mask)\n",
    "\n",
    "                if ((len(images_batch) >= self.batch_size) & (len(masks_batch) >= self.batch_size)):\n",
    "                    output_images_batch = images_batch[:]\n",
    "                    output_masks_batch = masks_batch[:]\n",
    "\n",
    "                    images_batch = []\n",
    "                    masks_batch = []\n",
    "\n",
    "                    yield {'image': tf.stack(output_images_batch), 'target': tf.stack(output_masks_batch)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self._generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41f57aa-f8d7-4311-a70a-573f9b67fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dataset/train/'\n",
    "custom_dataset = LabelDataset(path, 10).__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd942e-b9f5-4e68-be3b-0786d0f90e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-19 03:00:41.552761: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "unique_values_1 = set()\n",
    "unique_values_2 = set()\n",
    "\n",
    "unique_image_set_1 = []\n",
    "unique_image_set_2 = []\n",
    "unique_target_set_1 = []\n",
    "unique_target_set_2 = []\n",
    "\n",
    "\n",
    "for data in  custom_dataset:\n",
    "    for vector_indices in range(data['target'].shape[0]):\n",
    "        target_unique = set(np.unique(data['target'][vector_indices].numpy()))\n",
    "        if target_unique - unique_values_1:\n",
    "            unique_values_1 = unique_values_1.union(target_unique)\n",
    "            unique_image_set_1.append(data['image'][vector_indices])\n",
    "            unique_target_set_1.append(data['target'][vector_indices])\n",
    "            \n",
    "        elif target_unique - unique_values_2:\n",
    "            unique_values_2 = unique_values_2.union(target_unique)\n",
    "            unique_image_set_2.append(data['image'][vector_indices])\n",
    "            unique_target_set_2.append(data['target'][vector_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b95db-82e1-46be-bdb3-08f8116bf435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch_segments(target, image):\n",
    "    unique_batches = np.unique(target.numpy())\n",
    "    for batch_num in unique_batches:\n",
    "        if batch_num != 0:\n",
    "            # Create a mask for the current batch\n",
    "            batch_mask = tf.cast(target == batch_num, tf.float32)\n",
    "            \n",
    "            # Apply the mask to the image\n",
    "            masked_image = tf.cast(image, tf.float32) * tf.expand_dims(batch_mask, axis=-1)\n",
    "            \n",
    "            # Plot the masked image and the mask itself\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Changed subplots to 1 row and 3 columns\n",
    "            axes[0].imshow(tf.cast((image + 1) * (255 / 2), tf.uint8))  # Original image\n",
    "            axes[0].set_title(f\"Full Image\")\n",
    "            axes[1].imshow(tf.cast((masked_image + 1) * (255 / 2), tf.uint8))  # Masked image\n",
    "            axes[1].set_title(f\"Segment {batch_num} in Image\")\n",
    "            axes[2].imshow(batch_mask, cmap='gray')  # Mask\n",
    "            axes[2].set_title(f\"Segment {batch_num} Mask\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0c2d3-0de6-4219-b5e2-5f398bdf3eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_num in range(len(unique_image_set_2)):\n",
    "    visualize_batch_segments(unique_target_set_1[image_num], unique_image_set_1[image_num])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
